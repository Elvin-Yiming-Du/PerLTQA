# PerLTQA
PerLTQA is a new benchmark for memory classification, retrieval, and synthesis of Large Language Models.

## License

This dataset is licensed under the [CC BY-NC 4.0 License](https://creativecommons.org/licenses/by-nc/4.0/).  
It is intended for non-commercial research use only.

## Citation

If you use PerLTQA in your research, please cite our work:

```bibtex
@inproceedings{du-etal-2024-perltqa,
  title = {PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Fusion in Question Answering},
  author = {Du, Yiming and Wang, Hongru and Zhao, Zhengyi and Liang, Bin and Wang, Baojun and Zhong, Wanjun and Wang, Zezhong and Wong, Kam-Fai},
  booktitle = {Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10)},
  month = aug,
  year = {2024},
  address = {Bangkok, Thailand},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2024.sighan-1.18/},
  pages = {152--164}
}
